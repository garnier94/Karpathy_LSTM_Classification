{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char LSTM\n",
    "\n",
    "Objectif : \n",
    "\n",
    "- Apprendre à implémenter un LSTM en pytorch\n",
    "- Utiliser LSTM +DataLoader\n",
    "- Tester la différence entre une validation aléatoire & une validation sur période\n",
    "\n",
    "### I Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat =  open('input.txt', 'r')\n",
    "data = cat.read()\n",
    "data = list(map(ord, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_encoder = OrdinalEncoder()\n",
    "data = ord_encoder.fit_transform(np.array(data).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remy/.virtualenvs/pytorch_env/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1115394, 65)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder= OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data).todense()\n",
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 10000\n",
    "n_valid = 10000\n",
    "n_test = encoded_data.shape[0]-n_train-n_valid\n",
    "\n",
    "n_param = encoded_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II Modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramètres\n",
    "\n",
    "##LSTM\n",
    "input_size = n_param\n",
    "output_size = n_param\n",
    "hidden_size = 512\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "n_histo = 1\n",
    "temperature= 2.0\n",
    "\n",
    "\n",
    "##Learning\n",
    "learning_rate =  0.0001\n",
    "batch_size = 400\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_size\n",
    "        self.n_histo = n_histo\n",
    "        self.temperature = temperature\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size = hidden_size, num_layers = num_layers, dropout=dropout)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(num_layers, 1,self.hidden_layer_size).to(device),\n",
    "                            torch.zeros(num_layers, 1 ,self.hidden_layer_size).to(device))\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_layer_size, output_size)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        m = input_seq.shape[0]\n",
    "        l = []\n",
    "        for i in range(0, m-n_histo):\n",
    "            l.append(input_seq[i:i+n_histo,:])\n",
    "        dat = torch.stack(l,1)\n",
    "         \"\"\"\n",
    "        \n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(-1,1, n_param), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions /temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM().cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=  encoded_data[:n_train,:]\n",
    "training_ds = TensorDataset(torch.tensor(training_set[:-1,:], dtype = torch.float32),torch.tensor(data[1:n_train],dtype = torch.int64))\n",
    "training_dl = DataLoader(training_ds,batch_size=batch_size , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set=  encoded_data[n_train:n_train+n_valid,:]\n",
    "X_valid = torch.tensor(valid_set[:-1,:], dtype = torch.float32)\n",
    "y_valid = torch.tensor(data[n_train+n_histo:n_train+n_valid],dtype = torch.int64)[:,0]\n",
    "X_train =  torch.tensor(training_set[:-1,:], dtype = torch.float32)\n",
    "y_train =  torch.tensor(data[n_histo:n_train],dtype = torch.int64)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 loss: 4.17813921 Valid loss: 4.17753363\n",
      "epoch:   1 loss: 3.72119665 Valid loss: 3.75880098\n",
      "epoch:   2 loss: 3.26991439 Valid loss: 3.39874411\n",
      "epoch:   3 loss: 3.24033117 Valid loss: 3.37985468\n",
      "epoch:   4 loss: 3.23191714 Valid loss: 3.37617826\n",
      "epoch:   5 loss: 3.22818565 Valid loss: 3.37519884\n",
      "epoch:   6 loss: 3.22606707 Valid loss: 3.37514424\n",
      "epoch:   7 loss: 3.22461200 Valid loss: 3.37577391\n",
      "epoch:   8 loss: 3.22362781 Valid loss: 3.37634087\n",
      "epoch:   9 loss: 3.22295642 Valid loss: 3.37691116\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    y_pred_train = model(X_train.to(device))\n",
    "    single_loss = loss_function(y_pred_train, y_train.to(device))\n",
    "    y_pred_valid = model(X_valid.to(device))\n",
    "    valid_loss = loss_function(y_pred_valid, y_valid.to(device))\n",
    "    \n",
    "    print(f'epoch: {i:3} loss: {single_loss.item():10.8f} Valid loss: {valid_loss.item():10.8f}')\n",
    "    \n",
    "    for seq, labels in training_dl:\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(num_layers,1,model.hidden_layer_size).to(device), torch.zeros(num_layers,1,model.hidden_layer_size).to(device))\n",
    "        y_pred = model(seq.to(device))\n",
    "        single_loss = loss_function(y_pred, labels[:,0].to(device))\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_txt(seq):\n",
    "    cate = ord_encoder.inverse_transform(encoder.inverse_transform(np.array(seq)))\n",
    "    txt = ''.join(list(map(chr,cate)))\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e heart of generosity,\n",
      "And make bold power look pale--they threw their caps\n",
      "As they would hang them on the horns o' the moon,\n",
      "Shouting their emulation.\n",
      "\n",
      "MENENIUS:\n",
      "What is granted them?\n",
      "\n",
      "MARCIUS:\n",
      "Five tribunes to defend their vulgar wisdoms,\n",
      "Of their own choice: one's Junius Brutus,\n",
      "Sicinius Velutus, and I know not--'Sdeath!\n",
      "The rabble should have first unroof'd the city,\n",
      "Ere so prevail'd with me:\n"
     ]
    }
   ],
   "source": [
    "a= decode_txt(seq)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remy/.virtualenvs/pytorch_env/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    next_chr =  model(seq.to(device))\n",
    "    arr = F.softmax(next_chr[-1,:]).detach().cpu().numpy()\n",
    "    i =  np.argmax(arr)\n",
    "    a = torch.zeros(1,65)\n",
    "    a[0,i] = 1\n",
    "    seq = torch.cat([seq, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e heart of generosity,\n",
      "And make bold power look pale--they threw their caps\n",
      "As they would hang them on the horns o' the moon,\n",
      "Shouting their emulation.\n",
      "\n",
      "MENENIUS:\n",
      "What is granted them?\n",
      "\n",
      "MARCIUS:\n",
      "Five tribunes to defend their vulgar wisdoms,\n",
      "Of their own choice: one's Junius Brutus,\n",
      "Sicinius Velutus, and I know not--'Sdeath!\n",
      "The rabble should have first unroof'd the city,\n",
      "Ere so prevail'd with me:                              \n"
     ]
    }
   ],
   "source": [
    "a= decode_txt(seq)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"e heart of generosity,\\nAnd make bold power look pale--they threw their caps\\nAs they would hang them on the horns o' the moon,\\nShouting their emulation.\\n\\nMENENIUS:\\nWhat is granted them?\\n\\nMARCIUS:\\nFive tribunes to defend their vulgar wisdoms,\\nOf their own choice: one's Junius Brutus,\\nSicinius Velutus, and I know not--'Sdeath!\\nThe rabble should have first unroof'd the city,\\nEre so prevail'd with me:                              \""
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques tests avec les tenseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 1, 0],\n",
       "        [4, 2, 0],\n",
       "        [3, 3, 0],\n",
       "        [2, 4, 0],\n",
       "        [4, 5, 0],\n",
       "        [4, 6, 0],\n",
       "        [3, 7, 0],\n",
       "        [2, 8, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor([[4,1,0],[4,2,0],[3,3,0],[2,4,0],[4,5,0],[4,6,0],[3,7,0],[2,8,0]])\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 1, 0],\n",
       "         [4, 2, 0],\n",
       "         [3, 3, 0],\n",
       "         [2, 4, 0],\n",
       "         [4, 5, 0],\n",
       "         [4, 6, 0]],\n",
       "\n",
       "        [[4, 2, 0],\n",
       "         [3, 3, 0],\n",
       "         [2, 4, 0],\n",
       "         [4, 5, 0],\n",
       "         [4, 6, 0],\n",
       "         [3, 7, 0]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=2\n",
    "m = tt.shape[0]\n",
    "l = []\n",
    "for i in range(0, m-h):\n",
    "    l.append(tt[i:i+h,:])\n",
    "torch.stack(l,1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
